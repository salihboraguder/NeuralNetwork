{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def relu(self,X):\n",
    "        return np.maximum(0,X)\n",
    "    \n",
    "    def linear(self,X):\n",
    "        return X\n",
    "    \n",
    "    def sigmoid(self,X):\n",
    "        return 1/(1 + np.exp(-X))\n",
    "\n",
    "    def __init__(self,unit_num,act_func,input_size) -> None:\n",
    "\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.unit_num = unit_num\n",
    "        self.act_func = act_func\n",
    "        self.weights = np.random.rand(unit_num,input_size)*0.2\n",
    "        self.bias = np.zeros((unit_num,1))\n",
    "\n",
    "\n",
    "\n",
    "    def forward_prop(self,inp):\n",
    "        if self.act_func == \"sigmoid\":\n",
    "            \n",
    "            linear_output = np.dot(self.weights,inp) + self.bias\n",
    "\n",
    "            activation_output = self.sigmoid(linear_output)\n",
    "\n",
    "            return linear_output,activation_output\n",
    "        \n",
    "        elif self.act_func == \"relu\":\n",
    "            linear_output = np.dot(self.weights,inp) + self.bias\n",
    "\n",
    "            activation_output = self.relu(linear_output)\n",
    "\n",
    "            return linear_output, activation_output\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Layer with {self.unit_num} units and {self.act_func} activation function.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "\n",
    "    def __init__(self,X,Y) -> None:\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.layers = []\n",
    "        return None\n",
    "    \n",
    "    def add_layer(self,input_size,activation_function,layer_size):\n",
    "\n",
    "\n",
    "        if (len(self.layers) == 0) and (input_size != self.X.shape[0]):\n",
    "            raise Exception(\"First layer's input size should match with the number of features!\")\n",
    "        \n",
    "        elif len(self.layers) == 0:\n",
    "            new_layer = Layer(unit_num=layer_size,act_func = activation_function,input_size=input_size)\n",
    "            self.layers.append(new_layer)\n",
    "            print(\"Added a layer!\")\n",
    "            return None\n",
    "\n",
    "        elif input_size == self.layers[-1].unit_num:\n",
    "            new_layer = Layer(unit_num=layer_size,act_func = activation_function,input_size=input_size)\n",
    "            self.layers.append(new_layer)\n",
    "            print(\"Added a layer!\")\n",
    "            return None\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Input Size don't match with the last layer's output size!\")\n",
    "\n",
    "\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "\n",
    "        print(f\"Neural Network with {len(self.layers)} layers.\")\n",
    "        for _,layer in enumerate(self.layers):\n",
    "            print(f\"LAYER {_}: {layer}\")\n",
    "        \n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    def forward_propagation(self,X):\n",
    "        caches = [self.layers[0].forward_prop(X)]\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "        for i in range(1,len(self.layers)):\n",
    "            caches.append(self.layers[i].forward_prop(caches[i-1][1]))\n",
    "            #print(f\"Output of layer {i+1}:\\n{caches[i]}\")\n",
    "            #print(f\"Shape of layer {i}: {len(caches[i])}\")\n",
    "        \n",
    "        return caches\n",
    "\n",
    "\n",
    "    def sigmoid_back(self,dA,cache):\n",
    "        Z = cache\n",
    "        s= 1/(1+np.exp(-Z))\n",
    "\n",
    "        dZ = dA * s * (1-s)\n",
    "\n",
    "        #print(f\"dZ shape after sigmoid back: {dZ.shape}\")\n",
    "        \n",
    "\n",
    "        return dZ\n",
    "    \n",
    "\n",
    "    def relu_back(seld,dA,cache):\n",
    "        \n",
    "\n",
    "        Z = cache\n",
    "        \n",
    "\n",
    "        dZ = np.copy(dA)\n",
    "\n",
    "        dZ[Z <= 0] = 0\n",
    "\n",
    "        return dZ\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def linear_backward(self,dZ,W,b,A_prev):\n",
    "\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        dW = (1/m) * np.dot(dZ,A_prev.T)\n",
    "\n",
    "        db = (1/m) * np.sum(dZ,axis=1,keepdims=True)\n",
    "        \n",
    "\n",
    "        dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "\n",
    "        '''\n",
    "        print(f\"A_prev shape: {A_prev.shape}\")\n",
    "\n",
    "        print(f\"dA_prev shape: {dA_prev.shape}\")\n",
    "\n",
    "        print(f\"dW shape: {dW.shape}\")\n",
    "        '''\n",
    "\n",
    "        return dA_prev, db, dW\n",
    "    \n",
    "\n",
    "\n",
    "    def linear_activation_backward(self,dA,activation,cache,W,b,l):\n",
    "\n",
    "\n",
    "\n",
    "        linear_cache, activation_cache = cache\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            dZ = self.sigmoid_back(dA,activation_cache)\n",
    "            print(f\"dZ shape of layer {l}: {dZ.shape}\")\n",
    "            dA_prev, db, dW = self.linear_backward(dZ,W,b,linear_cache)\n",
    "\n",
    "\n",
    "\n",
    "        elif activation ==\"relu\":\n",
    "\n",
    "            dZ = self.relu_back(dA,activation_cache)\n",
    "            print(f\"dZ shape of layer {l}: {dZ.shape}\")\n",
    "\n",
    "            dA_prev, db,dW = self.linear_backward(dZ,W,b,linear_cache)\n",
    "        \n",
    "\n",
    "        return dA_prev, dW, db    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def compute_cost(self,cost_func,yhat,epsilon):\n",
    "\n",
    "        if cost_func ==\"crossentropy\":\n",
    "            \n",
    "            cost = np.sum((1-self.Y) * (-np.log(1-yhat + epsilon)) - self.Y * (np.log(yhat + epsilon)))/len(self.Y)\n",
    "            print(cost)\n",
    "            return cost\n",
    "        else:\n",
    "            return f\"No such a function as {cost_func}!\"\n",
    "        \n",
    "\n",
    "    def predict(self,X):\n",
    "\n",
    "        predictions = self.forward_propagation(X)[-1][-1]\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def backprop(self,Y,caches):\n",
    "\n",
    "\n",
    "        '''\n",
    "        AL: output of forward propagation\n",
    "        Y : true values\n",
    "        caches:evlayersery cache of \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        AL = caches[-1][1]\n",
    "        grads = {}\n",
    "\n",
    "        L = len(self.layers)\n",
    "\n",
    "        \n",
    "\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "\n",
    "\n",
    "        current_cache = caches[L-1]\n",
    "\n",
    "        #print(f\"FOR LAYER {L-1}\")\n",
    "\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)],grads[\"db\" + str(L)] = self.linear_activation_backward(dA = dAL,\n",
    "                                                               activation=self.layers[-1].act_func,\n",
    "                                                               cache=current_cache,\n",
    "                                                               W = self.layers[-1].weights,\n",
    "                                                               b = self.layers[-1].bias,l=L-1)\n",
    "        \n",
    "\n",
    "\n",
    "        for l in reversed(range(L-1)):\n",
    "            print(l)\n",
    "            \n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = self.linear_activation_backward(dA = grads[\"dA\" + str(l+1)],\n",
    "                                                                             activation = self.layers[l].act_func,\n",
    "                                                                             cache = current_cache,\n",
    "                                                                             W = self.layers[l].weights,\n",
    "                                                                             b = self.layers[l].bias,l=l)\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l+1)] = dW_temp\n",
    "            grads[\"db\" + str(l+1)] = db_temp\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "        for key in grads.keys():\n",
    "            print(f\"{key} shape: {grads[key].shape}\")\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "        return grads\n",
    "\n",
    "    \n",
    "    def update_params(self, grads,alpha):\n",
    "\n",
    "        \n",
    "        '''\n",
    "        alpha: learning rate\n",
    "        '''\n",
    "        L = len(self.layers)\n",
    "\n",
    "        \n",
    "\n",
    "    # Update rule for each parameter\n",
    "        for l in range(len(self.layers)):\n",
    "            print(f\"Weights shape: {self.layers[l].weights.shape} --- Bias shape: {self.layers[l].bias.shape}\")\n",
    "            print(f\"grads[dW{str(l+1)}] shape: {grads['dW' + str(l+1)].shape}\")\n",
    "            self.layers[l].weights = self.layers[l].weights - alpha * grads[\"dW\" + str(l+1)]\n",
    "            \n",
    "            self.layers[l].bias = self.layers[l].bias - alpha * grads[\"db\" + str(l+1)]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        return None\n",
    "    \n",
    "\n",
    "    def train(self,X,Y,alpha,max_iter):\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "            forward_cache = self.forward_propagation(X)\n",
    "            backprop_grads = self.backprop(Y,forward_cache)\n",
    "            self.update_params(backprop_grads,alpha)\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"COST AFTER ITERATION {iteration}: {self.compute_cost('crossentropy',Y,forward_cache[-1][1],1e-8)}\")\n",
    "\n",
    "        \n",
    "\n",
    "        print(\"Training is done!\")\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X SET SHAPE: (3, 4)\n",
      "Y SET SHAPE: (1, 4)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1,3,4,5],[2,5,6,7],[3,4,2,3]])\n",
    "Y = np.array([[0,1,0,1]])\n",
    "\n",
    "print(f\"X SET SHAPE: {X.shape}\")\n",
    "print(f\"Y SET SHAPE: {Y.shape}\")\n",
    "\n",
    "\n",
    "deneme = Layer(4,\"sigmoid\",3)\n",
    "deneme2 = Layer(10,\"sigmoid\",4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_deneme = NN(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added a layer!\n",
      "Added a layer!\n",
      "Added a layer!\n",
      "Added a layer!\n"
     ]
    }
   ],
   "source": [
    "nn_deneme.add_layer(3,\"sigmoid\",10)\n",
    "nn_deneme.add_layer(10,\"sigmoid\",20)\n",
    "nn_deneme.add_layer(20,\"relu\",4)\n",
    "nn_deneme.add_layer(4,\"sigmoid\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dZ shape of layer 3: (1, 4)\n",
      "2\n",
      "dZ shape of layer 2: (4, 4)\n",
      "1\n",
      "dZ shape of layer 1: (20, 4)\n",
      "0\n",
      "dZ shape of layer 0: (10, 4)\n",
      "dA3 shape: (4, 4)\n",
      "dW4 shape: (1, 1)\n",
      "db4 shape: (1, 1)\n",
      "dA2 shape: (20, 4)\n",
      "dW3 shape: (4, 4)\n",
      "db3 shape: (4, 1)\n",
      "dA1 shape: (10, 4)\n",
      "dW2 shape: (20, 20)\n",
      "db2 shape: (20, 1)\n",
      "dA0 shape: (3, 4)\n",
      "dW1 shape: (10, 10)\n",
      "db1 shape: (10, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dA3': array([[ 0.12258291, -0.05407539,  0.12521231, -0.05382131],\n",
       "        [ 0.10621257, -0.04685389,  0.10849082, -0.04663374],\n",
       "        [ 0.12670104, -0.05589203,  0.12941878, -0.05562942],\n",
       "        [ 0.10514294, -0.04638204,  0.10739825, -0.04616411]]),\n",
       " 'dW4': array([[0.16575724]]),\n",
       " 'db4': array([[0.20550736]]),\n",
       " 'dA2': array([[ 0.03955636, -0.01744962,  0.04040484, -0.01736763],\n",
       "        [ 0.04836273, -0.0213344 ,  0.0494001 , -0.02123416],\n",
       "        [ 0.04818614, -0.0212565 ,  0.04921973, -0.02115663],\n",
       "        [ 0.04829863, -0.02130613,  0.04933463, -0.02120602],\n",
       "        [ 0.02593854, -0.01144235,  0.02649492, -0.01138859],\n",
       "        [ 0.02539087, -0.01120076,  0.0259355 , -0.01114813],\n",
       "        [ 0.06116327, -0.02698115,  0.06247522, -0.02685437],\n",
       "        [ 0.05292664, -0.0233477 ,  0.05406191, -0.023238  ],\n",
       "        [ 0.03878284, -0.0171084 ,  0.03961473, -0.01702801],\n",
       "        [ 0.05853995, -0.02582391,  0.05979563, -0.02570258],\n",
       "        [ 0.04170076, -0.01839559,  0.04259524, -0.01830915],\n",
       "        [ 0.02306714, -0.01017568,  0.02356193, -0.01012787],\n",
       "        [ 0.05131271, -0.02263574,  0.05241337, -0.02252938],\n",
       "        [ 0.06051172, -0.02669373,  0.06180969, -0.0265683 ],\n",
       "        [ 0.04727258, -0.0208535 ,  0.04828658, -0.02075552],\n",
       "        [ 0.05477084, -0.02416123,  0.05594567, -0.02404771],\n",
       "        [ 0.04429695, -0.01954085,  0.04524712, -0.01944904],\n",
       "        [ 0.04708536, -0.02077092,  0.04809534, -0.02067332],\n",
       "        [ 0.03786517, -0.01670358,  0.03867737, -0.0166251 ],\n",
       "        [ 0.0393768 , -0.01737041,  0.04022143, -0.0172888 ]]),\n",
       " 'dW3': array([[0.04847267, 0.04775725, 0.04011734, 0.04015253],\n",
       "        [0.04199938, 0.04137951, 0.03475986, 0.03479036],\n",
       "        [0.0501011 , 0.04936164, 0.04146507, 0.04150145],\n",
       "        [0.04157642, 0.04096279, 0.03440981, 0.03444   ]]),\n",
       " 'db3': array([[0.03497463],\n",
       "        [0.03030394],\n",
       "        [0.03614959],\n",
       "        [0.02999876]]),\n",
       " 'dA1': array([[ 0.02174013, -0.00950809,  0.02200597, -0.00942993],\n",
       "        [ 0.02106359, -0.00920995,  0.02131713, -0.00913434],\n",
       "        [ 0.01983616, -0.00867722,  0.02008633, -0.00860791],\n",
       "        [ 0.01703594, -0.00745074,  0.01724681, -0.00739072],\n",
       "        [ 0.02027631, -0.00886511,  0.020519  , -0.0087919 ],\n",
       "        [ 0.01973859, -0.00863096,  0.01997578, -0.00855969],\n",
       "        [ 0.02005231, -0.00876804,  0.02029503, -0.00869701],\n",
       "        [ 0.01823799, -0.00797792,  0.01846607, -0.00791382],\n",
       "        [ 0.0178192 , -0.00779504,  0.0180417 , -0.0077321 ],\n",
       "        [ 0.01821328, -0.00796383,  0.01843176, -0.00789839]]),\n",
       " 'dW2': array([[0.00196544, 0.00144137, 0.00168118, 0.00143079, 0.00232764,\n",
       "         0.00163522, 0.0010764 , 0.00184699, 0.00147425, 0.00160985,\n",
       "         0.00139629, 0.00170331, 0.00150069, 0.00175208, 0.00166088,\n",
       "         0.00165974, 0.00166925, 0.00137303, 0.00101756, 0.00125263],\n",
       "        [0.00243882, 0.00178853, 0.00208611, 0.0017754 , 0.00288827,\n",
       "         0.00202908, 0.00133565, 0.00229185, 0.00182933, 0.00199757,\n",
       "         0.00173255, 0.00211357, 0.00186213, 0.00217407, 0.00206092,\n",
       "         0.0020595 , 0.00207131, 0.00170375, 0.00126264, 0.00155433],\n",
       "        [0.00241352, 0.00176998, 0.00206447, 0.00175698, 0.00285831,\n",
       "         0.00200803, 0.00132179, 0.00226807, 0.00181036, 0.00197685,\n",
       "         0.00171459, 0.00209164, 0.00184282, 0.00215152, 0.00203954,\n",
       "         0.00203813, 0.00204982, 0.00168607, 0.00124955, 0.00153821],\n",
       "        [0.00243635, 0.00178673, 0.002084  , 0.00177361, 0.00288535,\n",
       "         0.00202703, 0.00133429, 0.00228953, 0.00182749, 0.00199554,\n",
       "         0.00173079, 0.00211143, 0.00186025, 0.00217187, 0.00205883,\n",
       "         0.00205742, 0.00206921, 0.00170203, 0.00126137, 0.00155276],\n",
       "        [0.00127577, 0.00093559, 0.00109126, 0.00092872, 0.00151087,\n",
       "         0.00106142, 0.0006987 , 0.00119889, 0.00095693, 0.00104496,\n",
       "         0.00090636, 0.00110561, 0.0009741 , 0.00113729, 0.00107808,\n",
       "         0.00107733, 0.00108351, 0.00089123, 0.00066051, 0.00081309],\n",
       "        [0.0012734 , 0.00093386, 0.00108924, 0.00092701, 0.00150808,\n",
       "         0.00105946, 0.0006974 , 0.00119666, 0.00095516, 0.00104301,\n",
       "         0.00090464, 0.00110357, 0.00097229, 0.00113517, 0.00107608,\n",
       "         0.00107534, 0.00108151, 0.00088959, 0.00065928, 0.00081158],\n",
       "        [0.00311616, 0.0022853 , 0.0026655 , 0.0022685 , 0.00369047,\n",
       "         0.00259265, 0.00170658, 0.00292837, 0.00233742, 0.00255233,\n",
       "         0.00221366, 0.0027006 , 0.00237931, 0.00277787, 0.00263332,\n",
       "         0.00263151, 0.0026466 , 0.00217697, 0.00161331, 0.00198602],\n",
       "        [0.00263857, 0.00193502, 0.00225697, 0.00192082, 0.00312483,\n",
       "         0.00219527, 0.00144505, 0.00247956, 0.00197916, 0.00216119,\n",
       "         0.00187448, 0.00228667, 0.00201466, 0.00235214, 0.00222971,\n",
       "         0.00222818, 0.00224095, 0.00184329, 0.00136606, 0.00168164],\n",
       "        [0.00195393, 0.00143293, 0.00167134, 0.00142241, 0.00231402,\n",
       "         0.00162565, 0.00107009, 0.00183618, 0.00146562, 0.00160041,\n",
       "         0.00138809, 0.00169334, 0.0014919 , 0.00174182, 0.00165116,\n",
       "         0.00165002, 0.00165948, 0.00136501, 0.0010116 , 0.0012453 ],\n",
       "        [0.00293817, 0.00215474, 0.00251324, 0.00213892, 0.00347965,\n",
       "         0.00244454, 0.00160911, 0.0027611 , 0.0022039 , 0.00240656,\n",
       "         0.00208726, 0.00254632, 0.00224341, 0.0026192 , 0.00248289,\n",
       "         0.00248119, 0.00249541, 0.0020526 , 0.00152116, 0.00187258],\n",
       "        [0.00210578, 0.00154431, 0.00180123, 0.00153296, 0.00249387,\n",
       "         0.00175201, 0.00115324, 0.00197887, 0.00157954, 0.00172476,\n",
       "         0.0014959 , 0.00182496, 0.00160784, 0.00187717, 0.00177949,\n",
       "         0.00177827, 0.00178846, 0.00147111, 0.00109021, 0.00134207],\n",
       "        [0.00115463, 0.00084676, 0.00098764, 0.00084054, 0.00136741,\n",
       "         0.00096064, 0.00063235, 0.00108505, 0.00086607, 0.00094573,\n",
       "         0.00082027, 0.00100064, 0.00088161, 0.00102929, 0.00097572,\n",
       "         0.00097504, 0.00098063, 0.00080662, 0.00059779, 0.00073588],\n",
       "        [0.00258332, 0.00189451, 0.00220971, 0.0018806 , 0.00305941,\n",
       "         0.00214931, 0.00141478, 0.00242764, 0.00193773, 0.00211592,\n",
       "         0.0018352 , 0.0022388 , 0.00197247, 0.00230289, 0.00218303,\n",
       "         0.00218153, 0.00219404, 0.0018047 , 0.00133746, 0.00164643],\n",
       "        [0.00302488, 0.00221832, 0.0025874 , 0.00220204, 0.00358233,\n",
       "         0.00251667, 0.00165661, 0.00284258, 0.00226893, 0.00247759,\n",
       "         0.00214889, 0.00262146, 0.00230961, 0.00269651, 0.00255616,\n",
       "         0.0025544 , 0.00256904, 0.00211316, 0.00156606, 0.00192785],\n",
       "        [0.00236911, 0.00173741, 0.00202648, 0.00172465, 0.00280571,\n",
       "         0.00197108, 0.00129747, 0.00222634, 0.00177704, 0.00194047,\n",
       "         0.00168304, 0.00205315, 0.00180891, 0.00211193, 0.00200201,\n",
       "         0.00200063, 0.0020121 , 0.00165505, 0.00122655, 0.0015099 ],\n",
       "        [0.00274497, 0.00201304, 0.00234798, 0.00199827, 0.00325084,\n",
       "         0.00228379, 0.00150332, 0.00257955, 0.00205897, 0.00224834,\n",
       "         0.00195006, 0.00237888, 0.00209589, 0.00244699, 0.00231963,\n",
       "         0.00231803, 0.00233132, 0.00191762, 0.00142115, 0.00174945],\n",
       "        [0.00221945, 0.00162765, 0.00189847, 0.00161571, 0.00262847,\n",
       "         0.00184657, 0.00121551, 0.0020857 , 0.00166479, 0.0018179 ,\n",
       "         0.00157673, 0.00192345, 0.00169464, 0.00197852, 0.00187554,\n",
       "         0.00187425, 0.00188499, 0.0015505 , 0.00114907, 0.00141452],\n",
       "        [0.00237896, 0.00174464, 0.00203491, 0.00173183, 0.00281738,\n",
       "         0.00197928, 0.00130286, 0.0022356 , 0.00178443, 0.00194854,\n",
       "         0.00169002, 0.00206169, 0.00181643, 0.00212071, 0.00201034,\n",
       "         0.00200895, 0.00202047, 0.00166194, 0.00123165, 0.00151618],\n",
       "        [0.00193228, 0.00141707, 0.00165283, 0.00140666, 0.0022884 ,\n",
       "         0.00160766, 0.00105822, 0.00181584, 0.0014494 , 0.00158266,\n",
       "         0.00137265, 0.0016746 , 0.00147537, 0.00172251, 0.00163288,\n",
       "         0.00163176, 0.00164111, 0.0013499 , 0.00100039, 0.0012315 ],\n",
       "        [0.00199631, 0.00146402, 0.0017076 , 0.00145327, 0.00236422,\n",
       "         0.00166092, 0.00109329, 0.001876  , 0.00149742, 0.00163511,\n",
       "         0.00141816, 0.00173008, 0.00152426, 0.00177959, 0.00168698,\n",
       "         0.00168582, 0.00169548, 0.00139462, 0.00103354, 0.00127231]]),\n",
       " 'db2': array([[0.00251386],\n",
       "        [0.00311965],\n",
       "        [0.00308714],\n",
       "        [0.00311645],\n",
       "        [0.00163173],\n",
       "        [0.00162884],\n",
       "        [0.00398616],\n",
       "        [0.00337489],\n",
       "        [0.00249936],\n",
       "        [0.0037581 ],\n",
       "        [0.00269337],\n",
       "        [0.00147691],\n",
       "        [0.00330437],\n",
       "        [0.00386898],\n",
       "        [0.00303036],\n",
       "        [0.00351114],\n",
       "        [0.00283893],\n",
       "        [0.00304314],\n",
       "        [0.00247181],\n",
       "        [0.00255362]]),\n",
       " 'dA0': array([[ 0.00502625, -0.00209797,  0.00482466, -0.002032  ],\n",
       "        [ 0.00462499, -0.00192462,  0.00442714, -0.00186556],\n",
       "        [ 0.00372312, -0.00156431,  0.00363442, -0.00153161]]),\n",
       " 'dW1': array([[0.00052481, 0.00100456, 0.00042072, 0.00054806, 0.000679  ,\n",
       "         0.00073358, 0.00138186, 0.00107183, 0.00097317, 0.00082959],\n",
       "        [0.00049655, 0.00095173, 0.00040099, 0.0005224 , 0.00064497,\n",
       "         0.00069364, 0.00131012, 0.00101604, 0.00092442, 0.00078486],\n",
       "        [0.00048946, 0.00093225, 0.00038374, 0.00049967, 0.00062538,\n",
       "         0.00068462, 0.00127961, 0.00099241, 0.00089677, 0.00077236],\n",
       "        [0.0004197 , 0.00079892, 0.00032866, 0.00042791, 0.00053578,\n",
       "         0.00058684, 0.00109646, 0.00085022, 0.00076845, 0.00066179],\n",
       "        [0.00049196, 0.00093905, 0.00039013, 0.00050807, 0.00063247,\n",
       "         0.00068758, 0.00129037, 0.00100061, 0.00090688, 0.00077642],\n",
       "        [0.00046966, 0.00090067, 0.00037967, 0.00049466, 0.00061052,\n",
       "         0.00065631, 0.00123997, 0.0009618 , 0.00087487, 0.00074288],\n",
       "        [0.00046302, 0.00088784, 0.00037458, 0.00048801, 0.00060203,\n",
       "         0.00064678, 0.00122238, 0.00094801, 0.00086283, 0.00073199],\n",
       "        [0.00042961, 0.00082274, 0.00034568, 0.00045031, 0.00055687,\n",
       "         0.00060019, 0.00113215, 0.000878  , 0.00079821, 0.00067884],\n",
       "        [0.00042547, 0.00081283, 0.00033919, 0.00044176, 0.0005485 ,\n",
       "         0.0005943 , 0.00111747, 0.0008664 , 0.00078654, 0.0006713 ],\n",
       "        [0.00043187, 0.00082805, 0.00034883, 0.00045447, 0.00056113,\n",
       "         0.00060352, 0.0011399 , 0.00088417, 0.00080411, 0.00068306]]),\n",
       " 'db1': array([[0.00142194],\n",
       "        [0.00134369],\n",
       "        [0.00130527],\n",
       "        [0.001114  ],\n",
       "        [0.00131629],\n",
       "        [0.00127768],\n",
       "        [0.00125279],\n",
       "        [0.00115916],\n",
       "        [0.00113738],\n",
       "        [0.00117333]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_deneme.backprop(Y,nn_deneme.forward_propagation(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
